{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # obtains tokens with a least 1 alphabet\n",
    "    pattern = re.compile(r'[A-Za-z]+[\\w^\\']*|[\\w^\\']*[A-Za-z]+[\\w^\\']*')\n",
    "    return pattern.findall(text.lower())\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=\"V1 V2 V3 V1 V2 V3 V1 V2 V3 V4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['v1', 'v2', 'v3', 'v1', 'v2', 'v3', 'v1', 'v2', 'v3', 'v4']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id, id_to_word = mapping(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v2': 0, 'v4': 1, 'v1': 2, 'v3': 3}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'v2', 1: 'v4', 2: 'v1', 3: 'v3'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = generate_training_data(tokens, word_to_id, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 0, 0, 0, 3, 3, 3, 3, 2, 2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 3, 2,\n",
       "        2, 2, 2, 0, 0, 0, 0, 3, 3, 3, 1, 1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 2, 3, 2, 2, 0, 2, 0, 0, 3, 0, 3, 3, 2, 3, 2, 2, 0, 2, 0, 0,\n",
       "        3, 0, 3, 3, 2, 3, 1, 2, 0, 1, 0, 3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(id_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 34)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
       "        0., 0.],\n",
       "       [0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0.],\n",
       "       [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 1.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_one_hot[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encodeing of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "            \n",
    "#     if plot_cost:\n",
    "#         plt.plot(np.arange(epochs), costs)\n",
    "#         plt.xlabel('# of epochs')\n",
    "#         plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 5.531989247275737\n",
      "Cost after epoch 1: 5.531315821486185\n",
      "Cost after epoch 2: 5.530713268671494\n",
      "Cost after epoch 3: 5.530144281171688\n",
      "Cost after epoch 4: 5.5295128244333895\n",
      "Cost after epoch 5: 5.528881068672856\n",
      "Cost after epoch 6: 5.528250690166484\n",
      "Cost after epoch 7: 5.5276404906755365\n",
      "Cost after epoch 8: 5.526905910239879\n",
      "Cost after epoch 9: 5.526302170186733\n",
      "Cost after epoch 10: 5.525526894920241\n",
      "Cost after epoch 11: 5.5248199194242265\n",
      "Cost after epoch 12: 5.5240466117118165\n",
      "Cost after epoch 13: 5.523156460330774\n",
      "Cost after epoch 14: 5.522281438830421\n",
      "Cost after epoch 15: 5.521405191577828\n",
      "Cost after epoch 16: 5.520341746911295\n",
      "Cost after epoch 17: 5.5193188243755404\n",
      "Cost after epoch 18: 5.518196018274261\n",
      "Cost after epoch 19: 5.516977657924286\n",
      "Cost after epoch 20: 5.515662487320852\n",
      "Cost after epoch 21: 5.514153464163479\n",
      "Cost after epoch 22: 5.5127029799687195\n",
      "Cost after epoch 23: 5.511040425120801\n",
      "Cost after epoch 24: 5.5091793344437345\n",
      "Cost after epoch 25: 5.507283719808275\n",
      "Cost after epoch 26: 5.505195334641008\n",
      "Cost after epoch 27: 5.502880202780133\n",
      "Cost after epoch 28: 5.500408820208097\n",
      "Cost after epoch 29: 5.497707205106077\n",
      "Cost after epoch 30: 5.494751797162333\n",
      "Cost after epoch 31: 5.491712656957729\n",
      "Cost after epoch 32: 5.488276502914586\n",
      "Cost after epoch 33: 5.484883671094698\n",
      "Cost after epoch 34: 5.480741809603045\n",
      "Cost after epoch 35: 5.476756088754862\n",
      "Cost after epoch 36: 5.472277296067947\n",
      "Cost after epoch 37: 5.46773130523985\n",
      "Cost after epoch 38: 5.4621737952655645\n",
      "Cost after epoch 39: 5.456672456453795\n",
      "Cost after epoch 40: 5.450887402419803\n",
      "Cost after epoch 41: 5.444515005325693\n",
      "Cost after epoch 42: 5.437598807138829\n",
      "Cost after epoch 43: 5.4310119717740895\n",
      "Cost after epoch 44: 5.422886952987435\n",
      "Cost after epoch 45: 5.414724012152009\n",
      "Cost after epoch 46: 5.406458460373592\n",
      "Cost after epoch 47: 5.397592729814827\n",
      "Cost after epoch 48: 5.388391804967425\n",
      "Cost after epoch 49: 5.378059387656744\n",
      "Cost after epoch 50: 5.36764664427516\n",
      "Cost after epoch 51: 5.356421223144008\n",
      "Cost after epoch 52: 5.345780138489179\n",
      "Cost after epoch 53: 5.335415554770828\n",
      "Cost after epoch 54: 5.323126563377576\n",
      "Cost after epoch 55: 5.311455355525194\n",
      "Cost after epoch 56: 5.299103620079054\n",
      "Cost after epoch 57: 5.286183156104788\n",
      "Cost after epoch 58: 5.272638452835072\n",
      "Cost after epoch 59: 5.260698392754939\n",
      "Cost after epoch 60: 5.247537552373925\n",
      "Cost after epoch 61: 5.23455786326216\n",
      "Cost after epoch 62: 5.221034623926936\n",
      "Cost after epoch 63: 5.206976927158759\n",
      "Cost after epoch 64: 5.197687175197055\n",
      "Cost after epoch 65: 5.1856441443918575\n",
      "Cost after epoch 66: 5.173803237423732\n",
      "Cost after epoch 67: 5.162496665231268\n",
      "Cost after epoch 68: 5.15153092616347\n",
      "Cost after epoch 69: 5.1387939909142935\n",
      "Cost after epoch 70: 5.128402041785467\n",
      "Cost after epoch 71: 5.120956422332238\n",
      "Cost after epoch 72: 5.111911000935284\n",
      "Cost after epoch 73: 5.101851038551327\n",
      "Cost after epoch 74: 5.094922345153359\n",
      "Cost after epoch 75: 5.087418479668404\n",
      "Cost after epoch 76: 5.077579684775361\n",
      "Cost after epoch 77: 5.070884947015873\n",
      "Cost after epoch 78: 5.0658823888890065\n",
      "Cost after epoch 79: 5.057784942400006\n",
      "Cost after epoch 80: 5.056315292600409\n",
      "Cost after epoch 81: 5.04812638177026\n",
      "Cost after epoch 82: 5.043608510645864\n",
      "Cost after epoch 83: 5.036650205478143\n",
      "Cost after epoch 84: 5.033212486348715\n",
      "Cost after epoch 85: 5.025643094357292\n",
      "Cost after epoch 86: 5.02580285379716\n",
      "Cost after epoch 87: 5.015636352133502\n",
      "Cost after epoch 88: 5.010891277980359\n",
      "Cost after epoch 89: 5.01038725469916\n",
      "Cost after epoch 90: 5.003889301660793\n",
      "Cost after epoch 91: 5.001314223067826\n",
      "Cost after epoch 92: 4.992535289510222\n",
      "Cost after epoch 93: 4.990177756378444\n",
      "Cost after epoch 94: 4.986589424422825\n",
      "Cost after epoch 95: 4.981985222928086\n",
      "Cost after epoch 96: 4.976102659487282\n",
      "Cost after epoch 97: 4.969550461296928\n",
      "Cost after epoch 98: 4.968221828651795\n",
      "Cost after epoch 99: 4.961169873427779\n",
      "Cost after epoch 100: 4.954121944441508\n",
      "Cost after epoch 101: 4.952392671340242\n",
      "Cost after epoch 102: 4.945024977122569\n",
      "Cost after epoch 103: 4.941806119241323\n",
      "Cost after epoch 104: 4.938335948021139\n",
      "Cost after epoch 105: 4.928246155828175\n",
      "Cost after epoch 106: 4.925525341210202\n",
      "Cost after epoch 107: 4.92009278974159\n",
      "Cost after epoch 108: 4.911958091063123\n",
      "Cost after epoch 109: 4.906810670911368\n",
      "Cost after epoch 110: 4.903468880102568\n",
      "Cost after epoch 111: 4.893481678279448\n",
      "Cost after epoch 112: 4.890321906574192\n",
      "Cost after epoch 113: 4.883846834608123\n",
      "Cost after epoch 114: 4.879649195071931\n",
      "Cost after epoch 115: 4.872290244488702\n",
      "Cost after epoch 116: 4.864598219631099\n",
      "Cost after epoch 117: 4.855421899441972\n",
      "Cost after epoch 118: 4.853536855216292\n",
      "Cost after epoch 119: 4.846625817539308\n",
      "Cost after epoch 120: 4.842808979932634\n",
      "Cost after epoch 121: 4.834659640156843\n",
      "Cost after epoch 122: 4.824332669434341\n",
      "Cost after epoch 123: 4.820550006447107\n",
      "Cost after epoch 124: 4.814132380001838\n",
      "Cost after epoch 125: 4.810188770350925\n",
      "Cost after epoch 126: 4.80284165118012\n",
      "Cost after epoch 127: 4.800936138572946\n",
      "Cost after epoch 128: 4.791919045536281\n",
      "Cost after epoch 129: 4.784779967419569\n",
      "Cost after epoch 130: 4.780277408300826\n",
      "Cost after epoch 131: 4.775661318411193\n",
      "Cost after epoch 132: 4.761436684412008\n",
      "Cost after epoch 133: 4.759793602682891\n",
      "Cost after epoch 134: 4.754130143917232\n",
      "Cost after epoch 135: 4.751192006512269\n",
      "Cost after epoch 136: 4.743034433497867\n",
      "Cost after epoch 137: 4.732903588548837\n",
      "Cost after epoch 138: 4.73234623034736\n",
      "Cost after epoch 139: 4.722454472935765\n",
      "Cost after epoch 140: 4.7142855003815445\n",
      "Cost after epoch 141: 4.707235490356318\n",
      "Cost after epoch 142: 4.702551821364706\n",
      "Cost after epoch 143: 4.6953357016363215\n",
      "Cost after epoch 144: 4.691892560702284\n",
      "Cost after epoch 145: 4.682503728518133\n",
      "Cost after epoch 146: 4.674837387620697\n",
      "Cost after epoch 147: 4.675446406973369\n",
      "Cost after epoch 148: 4.6672206761797455\n",
      "Cost after epoch 149: 4.659572471968996\n",
      "Cost after epoch 150: 4.652668375936718\n",
      "Cost after epoch 151: 4.647117908421197\n",
      "Cost after epoch 152: 4.643049726006778\n",
      "Cost after epoch 153: 4.642720801362414\n",
      "Cost after epoch 154: 4.631044879862871\n",
      "Cost after epoch 155: 4.631669762705791\n",
      "Cost after epoch 156: 4.630002679381485\n",
      "Cost after epoch 157: 4.615164132239187\n",
      "Cost after epoch 158: 4.613317355716499\n",
      "Cost after epoch 159: 4.615385806195266\n",
      "Cost after epoch 160: 4.603960822769045\n",
      "Cost after epoch 161: 4.6001280871929495\n",
      "Cost after epoch 162: 4.599874616755402\n",
      "Cost after epoch 163: 4.5907763329278275\n",
      "Cost after epoch 164: 4.581743289653971\n",
      "Cost after epoch 165: 4.579567904268019\n",
      "Cost after epoch 166: 4.581869996372486\n",
      "Cost after epoch 167: 4.577533659715076\n",
      "Cost after epoch 168: 4.5747296697458015\n",
      "Cost after epoch 169: 4.5680923248811\n",
      "Cost after epoch 170: 4.561754308058142\n",
      "Cost after epoch 171: 4.554406592522971\n",
      "Cost after epoch 172: 4.554734393782029\n",
      "Cost after epoch 173: 4.544056552229538\n",
      "Cost after epoch 174: 4.547479511070513\n",
      "Cost after epoch 175: 4.545800806858219\n",
      "Cost after epoch 176: 4.540413458910392\n",
      "Cost after epoch 177: 4.532551954662114\n",
      "Cost after epoch 178: 4.532516115150315\n",
      "Cost after epoch 179: 4.528123453524268\n",
      "Cost after epoch 180: 4.522274621872503\n",
      "Cost after epoch 181: 4.517550662552349\n",
      "Cost after epoch 182: 4.522891602167763\n",
      "Cost after epoch 183: 4.511599232345362\n",
      "Cost after epoch 184: 4.514369297181954\n",
      "Cost after epoch 185: 4.502701843527164\n",
      "Cost after epoch 186: 4.505840841881797\n",
      "Cost after epoch 187: 4.501887899715553\n",
      "Cost after epoch 188: 4.496584993240568\n",
      "Cost after epoch 189: 4.493651065339867\n",
      "Cost after epoch 190: 4.4973283035715275\n",
      "Cost after epoch 191: 4.494275453282852\n",
      "Cost after epoch 192: 4.492639327880006\n",
      "Cost after epoch 193: 4.482398140655606\n",
      "Cost after epoch 194: 4.4789869834622795\n",
      "Cost after epoch 195: 4.483886616474342\n",
      "Cost after epoch 196: 4.477773576155307\n",
      "Cost after epoch 197: 4.472554231608523\n",
      "Cost after epoch 198: 4.476852755188852\n",
      "Cost after epoch 199: 4.4756513464472505\n",
      "Cost after epoch 200: 4.472063917529167\n",
      "Cost after epoch 201: 4.4706129899005385\n",
      "Cost after epoch 202: 4.468527087503159\n",
      "Cost after epoch 203: 4.463508971738214\n",
      "Cost after epoch 204: 4.46165122852465\n",
      "Cost after epoch 205: 4.463417803488479\n",
      "Cost after epoch 206: 4.458503677288714\n",
      "Cost after epoch 207: 4.459075295697711\n",
      "Cost after epoch 208: 4.454515653077278\n",
      "Cost after epoch 209: 4.452571588037323\n",
      "Cost after epoch 210: 4.444002191617267\n",
      "Cost after epoch 211: 4.4487375734029335\n",
      "Cost after epoch 212: 4.4454842137070605\n",
      "Cost after epoch 213: 4.445349076932474\n",
      "Cost after epoch 214: 4.440992047687376\n",
      "Cost after epoch 215: 4.436405289140169\n",
      "Cost after epoch 216: 4.440459094881053\n",
      "Cost after epoch 217: 4.441002383185308\n",
      "Cost after epoch 218: 4.433165094691003\n",
      "Cost after epoch 219: 4.428786388616539\n",
      "Cost after epoch 220: 4.4273870740934385\n",
      "Cost after epoch 221: 4.432997925680301\n",
      "Cost after epoch 222: 4.428486523685887\n",
      "Cost after epoch 223: 4.422986716414556\n",
      "Cost after epoch 224: 4.427099162118014\n",
      "Cost after epoch 225: 4.424244075326003\n",
      "Cost after epoch 226: 4.416724521930227\n",
      "Cost after epoch 227: 4.426983165120421\n",
      "Cost after epoch 228: 4.415124851443759\n",
      "Cost after epoch 229: 4.4151641434883215\n",
      "Cost after epoch 230: 4.421064951070319\n",
      "Cost after epoch 231: 4.41649090503876\n",
      "Cost after epoch 232: 4.411321259013272\n",
      "Cost after epoch 233: 4.41755164340902\n",
      "Cost after epoch 234: 4.412089475106622\n",
      "Cost after epoch 235: 4.411851786375901\n",
      "Cost after epoch 236: 4.4043514381234985\n",
      "Cost after epoch 237: 4.405646655583444\n",
      "Cost after epoch 238: 4.404757776939176\n",
      "Cost after epoch 239: 4.412397203252229\n",
      "Cost after epoch 240: 4.405621820428958\n",
      "Cost after epoch 241: 4.408794488629102\n",
      "Cost after epoch 242: 4.404314987956542\n",
      "Cost after epoch 243: 4.398223639339161\n",
      "Cost after epoch 244: 4.408110286255948\n",
      "Cost after epoch 245: 4.403327964472013\n",
      "Cost after epoch 246: 4.401765739022238\n",
      "Cost after epoch 247: 4.394126660935257\n",
      "Cost after epoch 248: 4.394551597572376\n",
      "Cost after epoch 249: 4.399672903754237\n",
      "Cost after epoch 250: 4.39281814565854\n",
      "Cost after epoch 251: 4.399020483672234\n",
      "Cost after epoch 252: 4.3921338145840245\n",
      "Cost after epoch 253: 4.393330954846487\n",
      "Cost after epoch 254: 4.389391219276758\n",
      "Cost after epoch 255: 4.397494265237359\n",
      "Cost after epoch 256: 4.38970773595831\n",
      "Cost after epoch 257: 4.3957691344904966\n",
      "Cost after epoch 258: 4.384146666742438\n",
      "Cost after epoch 259: 4.388925342410019\n",
      "Cost after epoch 260: 4.394804963793502\n",
      "Cost after epoch 261: 4.393626580623515\n",
      "Cost after epoch 262: 4.391702863034007\n",
      "Cost after epoch 263: 4.382235341340228\n",
      "Cost after epoch 264: 4.38751072471693\n",
      "Cost after epoch 265: 4.381125042789736\n",
      "Cost after epoch 266: 4.379381024302367\n",
      "Cost after epoch 267: 4.381829483248181\n",
      "Cost after epoch 268: 4.383954770957018\n",
      "Cost after epoch 269: 4.387727058073992\n",
      "Cost after epoch 270: 4.380153469650749\n",
      "Cost after epoch 271: 4.385368035583824\n",
      "Cost after epoch 272: 4.386625922967564\n",
      "Cost after epoch 273: 4.383722480350912\n",
      "Cost after epoch 274: 4.3789157190242\n",
      "Cost after epoch 275: 4.383779849403408\n",
      "Cost after epoch 276: 4.374164898862318\n",
      "Cost after epoch 277: 4.381260599475648\n",
      "Cost after epoch 278: 4.374628106649911\n",
      "Cost after epoch 279: 4.37113051364274\n",
      "Cost after epoch 280: 4.370783272526322\n",
      "Cost after epoch 281: 4.370618619894846\n",
      "Cost after epoch 282: 4.380994436566441\n",
      "Cost after epoch 283: 4.37062111356333\n",
      "Cost after epoch 284: 4.374638244287053\n",
      "Cost after epoch 285: 4.372942771259973\n",
      "Cost after epoch 286: 4.368642909971979\n",
      "Cost after epoch 287: 4.369378847073841\n",
      "Cost after epoch 288: 4.377550294316494\n",
      "Cost after epoch 289: 4.3755630020071745\n",
      "Cost after epoch 290: 4.377939522753506\n",
      "Cost after epoch 291: 4.366469453762819\n",
      "Cost after epoch 292: 4.377017633160979\n",
      "Cost after epoch 293: 4.373969946083513\n",
      "Cost after epoch 294: 4.377185992303783\n",
      "Cost after epoch 295: 4.375604426064982\n",
      "Cost after epoch 296: 4.370533911639009\n",
      "Cost after epoch 297: 4.369168762468904\n",
      "Cost after epoch 298: 4.368905860382053\n",
      "Cost after epoch 299: 4.367266069273778\n",
      "Cost after epoch 300: 4.373018744428043\n",
      "Cost after epoch 301: 4.373395933895471\n",
      "Cost after epoch 302: 4.36844110286011\n",
      "Cost after epoch 303: 4.373710241102873\n",
      "Cost after epoch 304: 4.369824284055436\n",
      "Cost after epoch 305: 4.369134380233607\n",
      "Cost after epoch 306: 4.361027345753384\n",
      "Cost after epoch 307: 4.368577846962758\n",
      "Cost after epoch 308: 4.364388541971302\n",
      "Cost after epoch 309: 4.366871146925788\n",
      "Cost after epoch 310: 4.362195977479768\n",
      "Cost after epoch 311: 4.366395648111652\n",
      "Cost after epoch 312: 4.369793216762266\n",
      "Cost after epoch 313: 4.367360564000437\n",
      "Cost after epoch 314: 4.3624394415922225\n",
      "Cost after epoch 315: 4.369912233696485\n",
      "Cost after epoch 316: 4.364173325688277\n",
      "Cost after epoch 317: 4.366302379640601\n",
      "Cost after epoch 318: 4.366010503441164\n",
      "Cost after epoch 319: 4.36966065711946\n",
      "Cost after epoch 320: 4.359616249826425\n",
      "Cost after epoch 321: 4.367325011911801\n",
      "Cost after epoch 322: 4.369011228252975\n",
      "Cost after epoch 323: 4.367809235560797\n",
      "Cost after epoch 324: 4.3685250791595145\n",
      "Cost after epoch 325: 4.359818273351823\n",
      "Cost after epoch 326: 4.364204253086867\n",
      "Cost after epoch 327: 4.3582580826712265\n",
      "Cost after epoch 328: 4.358461628850406\n",
      "Cost after epoch 329: 4.367305832756282\n",
      "Cost after epoch 330: 4.367060270815958\n",
      "Cost after epoch 331: 4.3579672744865405\n",
      "Cost after epoch 332: 4.361676981559352\n",
      "Cost after epoch 333: 4.358064271687915\n",
      "Cost after epoch 334: 4.361576429480723\n",
      "Cost after epoch 335: 4.360131155867687\n",
      "Cost after epoch 336: 4.363931731196789\n",
      "Cost after epoch 337: 4.36230935460174\n",
      "Cost after epoch 338: 4.367465655306923\n",
      "Cost after epoch 339: 4.365330379570131\n",
      "Cost after epoch 340: 4.3618004364880925\n",
      "Cost after epoch 341: 4.364898326488556\n",
      "Cost after epoch 342: 4.361275247375722\n",
      "Cost after epoch 343: 4.355384505281403\n",
      "Cost after epoch 344: 4.360820678298862\n",
      "Cost after epoch 345: 4.355535429936181\n",
      "Cost after epoch 346: 4.35657180881643\n",
      "Cost after epoch 347: 4.3579179036832265\n",
      "Cost after epoch 348: 4.366062009708383\n",
      "Cost after epoch 349: 4.364407562368501\n",
      "Cost after epoch 350: 4.364866086843564\n",
      "Cost after epoch 351: 4.366289039207983\n",
      "Cost after epoch 352: 4.360230958120995\n",
      "Cost after epoch 353: 4.365273378728619\n",
      "Cost after epoch 354: 4.36272540957941\n",
      "Cost after epoch 355: 4.365143047082874\n",
      "Cost after epoch 356: 4.358289109235261\n",
      "Cost after epoch 357: 4.363555541774995\n",
      "Cost after epoch 358: 4.356501013232054\n",
      "Cost after epoch 359: 4.360236492618398\n",
      "Cost after epoch 360: 4.363835730620263\n",
      "Cost after epoch 361: 4.355825761248083\n",
      "Cost after epoch 362: 4.35526562874943\n",
      "Cost after epoch 363: 4.357485319050962\n",
      "Cost after epoch 364: 4.3554231670383645\n",
      "Cost after epoch 365: 4.3564018163747145\n",
      "Cost after epoch 366: 4.356529813668589\n",
      "Cost after epoch 367: 4.363568253977059\n",
      "Cost after epoch 368: 4.364981865065851\n",
      "Cost after epoch 369: 4.357325293844176\n",
      "Cost after epoch 370: 4.363177846636274\n",
      "Cost after epoch 371: 4.359134495144134\n",
      "Cost after epoch 372: 4.366622165782418\n",
      "Cost after epoch 373: 4.356531271321909\n",
      "Cost after epoch 374: 4.359388330352911\n",
      "Cost after epoch 375: 4.36688577556648\n",
      "Cost after epoch 376: 4.365657431460271\n",
      "Cost after epoch 377: 4.359120069761058\n",
      "Cost after epoch 378: 4.366588140212022\n",
      "Cost after epoch 379: 4.366452717948658\n",
      "Cost after epoch 380: 4.366342415247945\n",
      "Cost after epoch 381: 4.36600479992371\n",
      "Cost after epoch 382: 4.355410252808072\n",
      "Cost after epoch 383: 4.3662193754396315\n",
      "Cost after epoch 384: 4.358978128938232\n",
      "Cost after epoch 385: 4.360735234148368\n",
      "Cost after epoch 386: 4.355831622998581\n",
      "Cost after epoch 387: 4.363420969024849\n",
      "Cost after epoch 388: 4.3560583218976845\n",
      "Cost after epoch 389: 4.3619773520115475\n",
      "Cost after epoch 390: 4.3660738439865305\n",
      "Cost after epoch 391: 4.366579426847397\n",
      "Cost after epoch 392: 4.359437961895306\n",
      "Cost after epoch 393: 4.363305724113271\n",
      "Cost after epoch 394: 4.357077670882085\n",
      "Cost after epoch 395: 4.3661357681737964\n",
      "Cost after epoch 396: 4.35640452941121\n",
      "Cost after epoch 397: 4.365252544207959\n",
      "Cost after epoch 398: 4.358513007522297\n",
      "Cost after epoch 399: 4.363561856866105\n",
      "Cost after epoch 400: 4.35808851372844\n",
      "Cost after epoch 401: 4.356812319510187\n",
      "Cost after epoch 402: 4.36031014909697\n",
      "Cost after epoch 403: 4.364125772243372\n",
      "Cost after epoch 404: 4.357915470962705\n",
      "Cost after epoch 405: 4.360172515843803\n",
      "Cost after epoch 406: 4.362231906950712\n",
      "Cost after epoch 407: 4.364367498027057\n",
      "Cost after epoch 408: 4.3660891807819135\n",
      "Cost after epoch 409: 4.364250464637186\n",
      "Cost after epoch 410: 4.357461975673689\n",
      "Cost after epoch 411: 4.35766056237126\n",
      "Cost after epoch 412: 4.364623221169481\n",
      "Cost after epoch 413: 4.357893037521313\n",
      "Cost after epoch 414: 4.367356614100515\n",
      "Cost after epoch 415: 4.366551498403245\n",
      "Cost after epoch 416: 4.367844183639295\n",
      "Cost after epoch 417: 4.364573830263036\n",
      "Cost after epoch 418: 4.364579221600636\n",
      "Cost after epoch 419: 4.366318319123063\n",
      "Cost after epoch 420: 4.3598441984489\n",
      "Cost after epoch 421: 4.358042666326197\n",
      "Cost after epoch 422: 4.3583064957308295\n",
      "Cost after epoch 423: 4.36761561212556\n",
      "Cost after epoch 424: 4.359305932813398\n",
      "Cost after epoch 425: 4.361954576991667\n",
      "Cost after epoch 426: 4.362095636601615\n",
      "Cost after epoch 427: 4.36572241829786\n",
      "Cost after epoch 428: 4.366826239661268\n",
      "Cost after epoch 429: 4.367500347297992\n",
      "Cost after epoch 430: 4.366089203168147\n",
      "Cost after epoch 431: 4.366031796401646\n",
      "Cost after epoch 432: 4.366923813062589\n",
      "Cost after epoch 433: 4.359468597194679\n",
      "Cost after epoch 434: 4.367805005931355\n",
      "Cost after epoch 435: 4.368554165388759\n",
      "Cost after epoch 436: 4.3670665202865955\n",
      "Cost after epoch 437: 4.364844981071438\n",
      "Cost after epoch 438: 4.369342086850863\n",
      "Cost after epoch 439: 4.366224966707842\n",
      "Cost after epoch 440: 4.367843060739606\n",
      "Cost after epoch 441: 4.368441519519996\n",
      "Cost after epoch 442: 4.362799650501545\n",
      "Cost after epoch 443: 4.3644722214378024\n",
      "Cost after epoch 444: 4.366538734482356\n",
      "Cost after epoch 445: 4.362779451824583\n",
      "Cost after epoch 446: 4.364506268792011\n",
      "Cost after epoch 447: 4.366719390016484\n",
      "Cost after epoch 448: 4.361910493002643\n",
      "Cost after epoch 449: 4.362619080346432\n",
      "Cost after epoch 450: 4.36346268955381\n",
      "Cost after epoch 451: 4.370196346874536\n",
      "Cost after epoch 452: 4.362444759576746\n",
      "Cost after epoch 453: 4.367773829457963\n",
      "Cost after epoch 454: 4.370606980814808\n",
      "Cost after epoch 455: 4.36555333740744\n",
      "Cost after epoch 456: 4.365584529376903\n",
      "Cost after epoch 457: 4.3706976559455\n",
      "Cost after epoch 458: 4.368036414909624\n",
      "Cost after epoch 459: 4.370147097255105\n",
      "Cost after epoch 460: 4.362512989797887\n",
      "Cost after epoch 461: 4.36622098935031\n",
      "Cost after epoch 462: 4.371087831208232\n",
      "Cost after epoch 463: 4.371109350709469\n",
      "Cost after epoch 464: 4.369219012199435\n",
      "Cost after epoch 465: 4.362332316847027\n",
      "Cost after epoch 466: 4.367298888683932\n",
      "Cost after epoch 467: 4.370074981129747\n",
      "Cost after epoch 468: 4.365483003478345\n",
      "Cost after epoch 469: 4.369111114900228\n",
      "Cost after epoch 470: 4.363696759156461\n",
      "Cost after epoch 471: 4.370047901861142\n",
      "Cost after epoch 472: 4.372019685340063\n",
      "Cost after epoch 473: 4.363351105744092\n",
      "Cost after epoch 474: 4.372293749683306\n",
      "Cost after epoch 475: 4.364880520870294\n",
      "Cost after epoch 476: 4.365072099423932\n",
      "Cost after epoch 477: 4.371947565268654\n",
      "Cost after epoch 478: 4.368063288999442\n",
      "Cost after epoch 479: 4.371423871946348\n",
      "Cost after epoch 480: 4.370204499236895\n",
      "Cost after epoch 481: 4.36865663663559\n",
      "Cost after epoch 482: 4.372824393094684\n",
      "Cost after epoch 483: 4.368356387173801\n",
      "Cost after epoch 484: 4.367169375366626\n",
      "Cost after epoch 485: 4.365310226813248\n",
      "Cost after epoch 486: 4.367533624273301\n",
      "Cost after epoch 487: 4.3674070700034395\n",
      "Cost after epoch 488: 4.366454410787263\n",
      "Cost after epoch 489: 4.366101459416258\n",
      "Cost after epoch 490: 4.366910696380509\n",
      "Cost after epoch 491: 4.366427566031825\n",
      "Cost after epoch 492: 4.36864268631085\n",
      "Cost after epoch 493: 4.374477449704262\n",
      "Cost after epoch 494: 4.372704178610858\n",
      "Cost after epoch 495: 4.366879848224732\n",
      "Cost after epoch 496: 4.373357689955224\n",
      "Cost after epoch 497: 4.368106660380841\n",
      "Cost after epoch 498: 4.367223993562844\n",
      "Cost after epoch 499: 4.369362743816193\n"
     ]
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 500, batch_size=10, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2's neighbor words: ['v1']\n",
      "v4's neighbor words: ['v3']\n",
      "v1's neighbor words: ['v3']\n",
      "v3's neighbor words: ['v1']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 3, 2]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_sorted_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2, 2, 3],\n",
       "       [1, 1, 1, 1],\n",
       "       [3, 0, 0, 0],\n",
       "       [2, 3, 3, 2]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, 3]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'v2', 1: 'v4', 2: 'v1', 3: 'v3'}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras['WRD_EMB'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 50)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras['W'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'WRD_EMB': array([[-0.54569614, -0.0380432 , -0.41602529,  0.70900913, -0.01911173,\n",
       "         -0.66307915,  0.1314658 , -0.92234908, -0.07331426, -0.64262143,\n",
       "         -0.10510148,  0.76957309,  0.30280831,  0.18995917, -0.68691935,\n",
       "         -0.39689627,  0.1847396 ,  0.72625545, -0.13159031, -0.37917101,\n",
       "          0.13890421,  0.38502223, -0.61587931, -0.19813591, -0.61027395,\n",
       "         -0.44750954,  0.42822442,  0.05292631,  0.02114448, -0.55452888,\n",
       "          0.29286201,  0.49168148, -0.5034531 ,  0.4204554 , -0.21743596,\n",
       "          0.15460933,  0.16580675, -0.56009311, -0.59979153, -0.09200651,\n",
       "         -0.92023894, -0.56448016,  0.09391948, -0.58702173,  0.05672181,\n",
       "          0.03275275, -0.47276156, -0.22793464, -0.76066527,  0.08153289],\n",
       "        [ 0.81453394, -0.14318716,  0.20034493, -0.56222349, -0.04659272,\n",
       "          0.53552213,  0.07397529,  0.89777691,  0.15077317,  0.37466312,\n",
       "          0.04265845, -0.63062387,  0.03950082,  0.29916115,  0.43349633,\n",
       "          0.10590618, -0.19911829, -0.66964006,  0.30456151,  0.10627646,\n",
       "         -0.15332911, -0.2695843 ,  0.47833798,  0.00986553,  0.70473673,\n",
       "          0.53992933, -0.27490911,  0.19339244, -0.10598871,  0.1476855 ,\n",
       "         -0.65357267, -0.39340515,  0.69012549, -0.28896356,  0.19852813,\n",
       "          0.10496095, -0.16397228,  0.56216792,  0.30119025,  0.3814755 ,\n",
       "          0.74461365,  0.28819463,  0.06065627,  0.51518863, -0.18729589,\n",
       "         -0.15017728,  0.13027014,  0.10148155,  0.47442765,  0.40147738],\n",
       "        [ 0.90961607, -0.08592783,  0.35365681, -0.79379754, -0.0137904 ,\n",
       "          0.75514044, -0.01662248,  1.20527475,  0.13718932,  0.61633556,\n",
       "          0.10190008, -0.90273952, -0.12555877,  0.13768822,  0.66786446,\n",
       "          0.24100773, -0.27320469, -0.88270471,  0.28737433,  0.27447999,\n",
       "         -0.16666926, -0.43795424,  0.67830967,  0.10725122,  0.86584751,\n",
       "          0.66657384, -0.42927485,  0.08487774, -0.10242558,  0.39413539,\n",
       "         -0.69214356, -0.55163141,  0.81092331, -0.4320182 ,  0.2483836 ,\n",
       "          0.0427999 , -0.19240493,  0.77638542,  0.54704122,  0.35122706,\n",
       "          1.024769  ,  0.51717022,  0.00380599,  0.69265813, -0.18196683,\n",
       "         -0.14701001,  0.29645877,  0.18010978,  0.72216719,  0.30772506],\n",
       "        [-0.75920293,  0.16891482, -0.24122427,  0.66564405,  0.10692733,\n",
       "         -0.51658569, -0.04219343, -0.69794494, -0.16955512, -0.37629514,\n",
       "          0.01127569,  0.60278873, -0.03284639, -0.30316124, -0.44419741,\n",
       "         -0.21206285,  0.06038144,  0.71406335, -0.38164824, -0.21284878,\n",
       "          0.30927   ,  0.27594632, -0.50748013, -0.05791711, -0.54408523,\n",
       "         -0.41585664,  0.17846582, -0.39491663,  0.10708095, -0.21893259,\n",
       "          0.59808687,  0.45520409, -0.65877892,  0.26646352, -0.25996582,\n",
       "          0.11624136,  0.08342111, -0.41036033, -0.28552552, -0.21007338,\n",
       "         -0.7860294 , -0.33841306, -0.09838896, -0.68516229,  0.13881067,\n",
       "          0.15676499, -0.04849548, -0.19869393, -0.59653347, -0.24075943]]),\n",
       " 'W': array([[-2.37544061e-01,  1.89007559e-01,  1.78445233e-01,\n",
       "         -5.90224797e-02,  8.98615090e-02,  9.70303066e-02,\n",
       "         -1.73691448e-01,  8.44314682e-02, -7.86134689e-02,\n",
       "          2.25887221e-01,  8.77560380e-02, -1.02236035e-01,\n",
       "         -3.10138101e-01, -4.62680349e-01,  2.08852303e-01,\n",
       "          2.21398011e-01, -4.46291992e-02,  2.59086623e-03,\n",
       "         -2.09094275e-01,  1.84900011e-01,  1.01862314e-01,\n",
       "         -7.68116149e-02,  8.43128200e-02,  1.39431976e-01,\n",
       "         -2.26986996e-02, -5.05393709e-02, -1.80104074e-01,\n",
       "         -3.42041246e-01,  8.43892555e-02,  3.24747368e-01,\n",
       "          3.44561200e-01, -3.77805036e-02, -1.86953063e-01,\n",
       "         -1.25603890e-01, -1.35182290e-02, -1.25907029e-01,\n",
       "         -4.02032387e-02,  3.82923102e-02,  2.64228031e-01,\n",
       "         -1.86098919e-01,  1.11925627e-01,  2.03724099e-01,\n",
       "         -1.77213479e-01, -5.86935362e-02,  1.03790109e-01,\n",
       "          1.26660284e-01,  3.74773268e-01,  6.38020771e-02,\n",
       "          1.77277209e-01, -3.80511134e-01],\n",
       "        [-3.05650053e-02, -5.51437955e-02,  4.05488945e-02,\n",
       "         -1.21240825e-01, -9.74320608e-02, -5.06721883e-02,\n",
       "         -1.41654708e-02, -2.89321059e-01,  4.65787999e-02,\n",
       "         -3.51272113e-02, -7.99887742e-02,  7.54453131e-02,\n",
       "          3.65202489e-02,  6.44857178e-02, -1.35454471e-02,\n",
       "          1.32458113e-01,  1.96586976e-01, -4.92946438e-02,\n",
       "          1.30748173e-01,  1.04442090e-01, -2.16835377e-01,\n",
       "          2.77669178e-02,  1.48038923e-02,  3.47818998e-02,\n",
       "         -1.99851603e-01, -1.69363230e-01,  1.49336129e-01,\n",
       "          3.08143635e-01, -8.26659721e-03,  4.00136453e-02,\n",
       "          5.63365741e-02, -6.62419005e-02, -2.87363580e-02,\n",
       "          4.02011043e-02,  9.31709909e-02, -2.74546137e-01,\n",
       "          1.02117402e-01, -2.35355959e-01, -6.54249196e-02,\n",
       "         -1.93120737e-01,  3.93631541e-02,  2.09929177e-02,\n",
       "          5.66123558e-02,  2.04731698e-01,  5.36580038e-02,\n",
       "         -1.05468509e-02, -1.18638492e-01,  1.20653704e-01,\n",
       "          1.34496325e-01, -1.72206196e-01],\n",
       "        [ 2.55341302e-03, -2.20311279e-02, -5.09035695e-02,\n",
       "          5.86587808e-02, -1.27583920e-02, -7.04716197e-02,\n",
       "          2.93326650e-02, -7.03749969e-02,  3.86399456e-03,\n",
       "         -8.37614104e-02, -7.54780471e-03,  7.45783620e-02,\n",
       "          6.51120318e-02,  7.70868612e-02, -8.97782316e-02,\n",
       "         -6.47163866e-02,  1.57600519e-02,  6.06104551e-02,\n",
       "          1.39483783e-02, -5.48792191e-02,  5.87287213e-03,\n",
       "          3.37730588e-02, -6.92573440e-02, -3.94686104e-02,\n",
       "         -3.45694183e-02, -2.75244196e-02,  6.18527092e-02,\n",
       "          2.98494024e-02, -1.43792199e-02, -9.05642450e-02,\n",
       "         -2.47796344e-02,  4.70817736e-02, -7.30230733e-03,\n",
       "          4.64515810e-02, -1.16580339e-02,  4.53977774e-02,\n",
       "          3.08326748e-02, -3.76955973e-02, -8.12213066e-02,\n",
       "          2.14950348e-02, -9.39766320e-02, -7.59141896e-02,\n",
       "          2.11189249e-02, -5.04799891e-02, -9.43823527e-03,\n",
       "         -1.31263376e-02, -9.06535480e-02, -2.90476204e-02,\n",
       "         -9.80332291e-02,  6.73213969e-02],\n",
       "        [ 3.11257856e-01, -1.02256983e-01, -1.42697219e-01,\n",
       "          8.96499854e-02,  5.19883197e-04,  3.74335745e-03,\n",
       "          1.47090591e-01,  2.82048854e-01,  3.44881323e-02,\n",
       "         -1.18155767e-01,  2.94576506e-02, -3.39242848e-02,\n",
       "          2.17489837e-01,  3.29961799e-01, -1.25267813e-01,\n",
       "         -2.69348077e-01, -1.60563298e-01, -1.37903203e-02,\n",
       "          7.42749638e-02, -2.23812380e-01,  1.01336964e-01,\n",
       "         -2.87070322e-04, -5.81840465e-02, -1.47096989e-01,\n",
       "          2.69887136e-01,  2.31117926e-01,  3.69784007e-04,\n",
       "         -1.20693862e-02, -7.82540318e-02, -2.84152803e-01,\n",
       "         -3.70570952e-01,  6.12481879e-02,  2.46988335e-01,\n",
       "          2.70643204e-02, -3.93800580e-02,  3.69594742e-01,\n",
       "         -4.19320219e-02,  2.31482110e-01, -1.18506461e-01,\n",
       "          3.45928095e-01, -6.83348325e-02, -1.55138855e-01,\n",
       "          7.63540057e-02, -1.11373850e-01, -1.39049978e-01,\n",
       "         -9.81910620e-02, -1.76510232e-01, -1.46978079e-01,\n",
       "         -2.32058505e-01,  4.89446775e-01]])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
